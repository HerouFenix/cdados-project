{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction - QSAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\"Mathematically, feature extraction can be seen as a transformation of the original data space, into a new one described by fewer variables, usually orthogonal among them.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a dimensionality reduction technique that identifies important relationships in our data, transforms the existing data based on these relationships, and then quantifies the importance of these relationships so we can keep the most important relationships and drop the others. The main idea behind PCA is to find the best possible variables: the ones that summarize all the data as well as only possible, among all conceivable linear combinations of the original ones, and that simultaneously allow for reconstructing the original data, also as well as possible. To remember this definition, we can break it down into four steps:\n",
    "\n",
    "* We identify the relationship among features through a Covariance Matrix.\n",
    "* Through the linear transformation or eigendecomposition of the Covariance Matrix, we get eigenvectors and eigenvalues.\n",
    "* Then we transform our data using Eigenvectors into principal components.\n",
    "* Lastly, we quantify the importance of these relationships using Eigenvalues and keep the important principal components.\n",
    "\n",
    "\n",
    "**Data Cleaning is Important**: PCA is sensitive to outliers and missing values. [source](https://towardsdatascience.com/feature-extraction-using-principal-component-analysis-a-simplified-visual-demo-e5592ced100a)\n",
    "\n",
    "**Standardize Data**: PCA uses Euclidean distance as its feature vector similarity metric, so make sure we scale the features before applying PCA. [source](https://towardsdatascience.com/feature-extraction-using-principal-component-analysis-a-simplified-visual-demo-e5592ced100a)\n",
    "\n",
    "\n",
    "That being said, Principal Component Analysis must **only** be performed on Data that has been standardized and have its outliers handled, since it is very sensitive to irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "\n",
    "import csv\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA, SparsePCA, IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "#  “Covariance” indicates the direction of the linear relationship between variables.\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_pca(X, y, filename):\n",
    "    \n",
    "    # Variance Explained\n",
    "    mean = (X.mean(axis=0)).tolist()\n",
    "    centered_data = X - mean\n",
    "    cov_mtx = centered_data.cov()\n",
    "    eigvals, eigvecs = np.linalg.eig(cov_mtx)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(centered_data)\n",
    "    PC = pca.components_\n",
    "    var = pca.explained_variance_\n",
    "    \n",
    "    # Plot the covariance matrix for the dataset\n",
    "    cov_mtx = np.round(cov_mtx, 3)\n",
    "    sns.heatmap(cov_mtx, annot=True, fmt='g')\n",
    "    plt.show()\n",
    "\n",
    "    # PLOT EXPLAINED VARIANCE RATIO\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.title('Explained variance ratio')\n",
    "    plt.xlabel('PC')\n",
    "    plt.ylabel('ratio')\n",
    "    x_values = [str(i) for i in range(1, len(pca.components_) + 1)]\n",
    "    bwidth = 0.5\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(x_values)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.bar(x_values, pca.explained_variance_ratio_, width=bwidth)\n",
    "    ax.plot(pca.explained_variance_ratio_)\n",
    "    for i, v in enumerate(pca.explained_variance_ratio_):\n",
    "        ax.text(i, v+0.05, f'{v*100:.1f}', ha='center', fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Cumulative Variance Explained\n",
    "    pca = PCA().fit(X)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    xi = np.arange(1, len(pca.components_) + 1, step=1)\n",
    "    yi = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, yi, marker='o', linestyle='--', color='b')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.xticks(np.arange(0, len(pca.components_) + 1, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The number of components needed to explain variance')\n",
    "\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()\n",
    "    \n",
    "    # Alternatively, sklearn.decomposition.PCA supports a parameter that allows for the desired Cov threshold\n",
    "    pca = PCA(n_components = 0.95).fit(X)\n",
    "\n",
    "    # Transform the data according to the previous fit\n",
    "    X_transformation = pca.transform(X)\n",
    "    \n",
    "    print(X_transformation.shape)\n",
    "    \n",
    "    # Merge results into a single dataframe\n",
    "    y = np.array(y).reshape((np.array(y).shape[0], 1))\n",
    "    to_be_df = np.append(X_transformation, y, axis=1)\n",
    "    column_names = [(\"Eigen\" + str(i)) for i in range (0, X_transformation.shape[1])] + [\"DEATH_EVENT\"]\n",
    "\n",
    "    store_data = pd.DataFrame(data=to_be_df, columns=column_names)\n",
    "    store_data.head()\n",
    "    \n",
    "    # Store DataFrame to a CSV\n",
    "    output_file = '../datasets/pca_output/qsar/' + filename.split('/')[-1].split('.')[0] + '_pca.csv'\n",
    "    store_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_kpca(X, y, filename):\n",
    "    \n",
    "    # Cumulative Variance Explained\n",
    "    kpca = KernelPCA(kernel='rbf')\n",
    "\n",
    "    kpca_transform = kpca.fit_transform(X)\n",
    "    explained_variance = np.var(kpca_transform, axis=0)\n",
    "    explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    yi = np.cumsum(explained_variance_ratio)\n",
    "    xi = np.arange(1, len(yi) + 1, step=1)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, yi, marker='o', linestyle='--', color='b')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.xticks(np.arange(0, len(yi), step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The number of components needed to explain variance')\n",
    "\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract ideal number of components\n",
    "    n_components = np.argmax(yi > 0.95) + 1\n",
    "    \n",
    "    # Alternatively, sklearn.decomposition.PCA supports a parameter that allows for the desired Cov threshold\n",
    "    pca = KernelPCA(n_components=n_components, kernel='rbf').fit(X)\n",
    "\n",
    "    # Transform the data according to the previous fit\n",
    "    X_transformation = pca.transform(X)\n",
    "    \n",
    "    # Merge results into a single dataframe\n",
    "    y = np.array(y).reshape((np.array(y).shape[0], 1))\n",
    "    to_be_df = np.append(X_transformation, y, axis=1)\n",
    "    column_names = [(\"Eigen\" + str(i)) for i in range (0, X_transformation.shape[1])] + [\"DEATH_EVENT\"]\n",
    "\n",
    "    store_data = pd.DataFrame(data=to_be_df, columns=column_names)\n",
    "    store_data.head()\n",
    "    \n",
    "    # Store DataFrame to a CSV\n",
    "    output_file = '../datasets/pca_output/qsar/' + filename.split('/')[-1].split('.')[0] + '_kpca.csv'\n",
    "    store_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_ipca(X, y, filename):\n",
    "    \n",
    "    # Cumulative Variance Explained\n",
    "    kpca = IncrementalPCA(batch_size=20)\n",
    "\n",
    "    kpca_transform = kpca.fit_transform(X)\n",
    "    explained_variance = np.var(kpca_transform, axis=0)\n",
    "    explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    yi = np.cumsum(explained_variance_ratio)\n",
    "    xi = np.arange(1, len(yi) + 1, step=1)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, yi, marker='o', linestyle='--', color='b')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.xticks(np.arange(0, len(yi), step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The number of components needed to explain variance')\n",
    "\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract ideal number of components\n",
    "    n_components = np.argmax(yi > 0.95) + 1\n",
    "    \n",
    "    # Alternatively, sklearn.decomposition.PCA supports a parameter that allows for the desired Cov threshold\n",
    "    pca = IncrementalPCA(n_components=n_components, batch_size=20).fit(X)\n",
    "\n",
    "    # Transform the data according to the previous fit\n",
    "    X_transformation = pca.transform(X)\n",
    "    \n",
    "    # Merge results into a single dataframe\n",
    "    y = np.array(y).reshape((np.array(y).shape[0], 1))\n",
    "    to_be_df = np.append(X_transformation, y, axis=1)\n",
    "    column_names = [(\"Eigen\" + str(i)) for i in range (0, X_transformation.shape[1])] + [\"DEATH_EVENT\"]\n",
    "\n",
    "    store_data = pd.DataFrame(data=to_be_df, columns=column_names)\n",
    "    store_data.head()\n",
    "    \n",
    "    # Store DataFrame to a CSV\n",
    "    output_file = '../datasets/pca_output/qsar/' + filename.split('/')[-1].split('.')[0] + '_ipca.csv'\n",
    "    store_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_lda(X, y, filename):\n",
    "    \n",
    "    lda = LDA(n_components=None)\n",
    "    lda.fit(X, y)\n",
    "    lda_var_ratios = lda.explained_variance_ratio_\n",
    "\n",
    "    n_components = select_n_components(lda_var_ratios, 0.95)\n",
    "    \n",
    "    lda = LDA(n_components=n_components)\n",
    "    X_transformation = lda.fit_transform(X, y)\n",
    "    \n",
    "    # Merge results into a single dataframe\n",
    "    y = np.array(y).reshape((np.array(y).shape[0], 1))\n",
    "    to_be_df = np.append(X_transformation, y, axis=1)\n",
    "    column_names = [(\"Eigen\" + str(i)) for i in range (0, X_transformation.shape[1])] + [\"DEATH_EVENT\"]\n",
    "\n",
    "    store_data = pd.DataFrame(data=to_be_df, columns=column_names)\n",
    "    store_data.head()\n",
    "    \n",
    "    output_file = '../datasets/pca_output/qsar/' + filename.split('/')[-1].split('.')[0] + '_lda.csv'\n",
    "    store_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_component_analysis(filename):\n",
    "    \n",
    "    print(\"ANALYZING:\", filename)\n",
    "    \n",
    "    data: pd.DataFrame = pd.read_csv(filename, sep=';', header=None)  \n",
    "    \n",
    "    y = np.array(data.pop(data.shape[1] - 1).values) # Target Variable\n",
    "    y = [1 if i == 'positive' else 0 for i in y]\n",
    "\n",
    "    X = pd.DataFrame(data.values) # Values of each feature on each record\n",
    "    X = X.astype(np.int64)\n",
    "    labels = pd.unique(y)\n",
    "        \n",
    "    feature_extraction_pca(X, y, filename)\n",
    "    feature_extraction_ipca(X, y, filename)\n",
    "    feature_extraction_lda(X, y, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING: ../datasets/pca_input/qsar/ORAL_anova.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = '../datasets/pca_input/qsar'\n",
    "\n",
    "overall_accs = []\n",
    "datasets = []\n",
    "    \n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        path = directory + '/' + filename\n",
    "        \n",
    "        perform_component_analysis(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
