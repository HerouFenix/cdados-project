{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "\"K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.\n",
    "\n",
    "Typically, unsupervised algorithms make inferences from datasets using only input vectors without referring to known, or labelled, outcomes.\"\n",
    "\n",
    "\"You’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster.\n",
    "\n",
    "Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares.\n",
    "\n",
    "In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.\n",
    "\n",
    "The ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Failure Prediction Dataset - Standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/hf_scaled/HF_standardized.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (11,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "print(db)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Yes, it is unlikely that binary data can be clustered satisfactorily. To see why, consider what happens as the K-Means algorithm processes cases.\n",
    "\n",
    "For binary data, the Euclidean distance measure used by K-Means reduces to counting the number of variables on which two cases disagree. After the initial centers are chosen (which depends on the order of the cases), the centers are still binary data. For the first iteration, as the cases are compared to cluster means, they will always be at some integer distance from each of the centers. There will often be ties, and the case will be assigned to a cluster in an arbitrary manner. Using Euclidean distance (the only measure available to K-Means), it is impossible to overcome the symmetry and break the ties in any meaningful way.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means No Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/hf_scaled/HF_standardized.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "numeric_vars = data.select_dtypes(include='number').columns\n",
    "\n",
    "numeric_data = data\n",
    "binary_data = data\n",
    "\n",
    "for n in range(len(numeric_vars)):\n",
    "    num_unique = len(list(set(data[numeric_vars[n]].values)))\n",
    "    if num_unique == 2:\n",
    "        numeric_data = numeric_data.drop(columns=[data.columns[n]], axis=1) #Remove binary columns\n",
    "\n",
    "    else:\n",
    "        binary_data = binary_data.drop(columns=[data.columns[n]], axis=1) #Remove non-binary columns\n",
    "\n",
    "print(numeric_data.head())\n",
    "data = numeric_data\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (6,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "print(db)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Failure Prediction Dataset - Standardized + ANOVA + FG + Outlier + Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/TO_TEST/HF/HF_S_FAnova_extra_outlierTrim_IQS_B.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Yes, it is unlikely that binary data can be clustered satisfactorily. To see why, consider what happens as the K-Means algorithm processes cases.\n",
    "\n",
    "For binary data, the Euclidean distance measure used by K-Means reduces to counting the number of variables on which two cases disagree. After the initial centers are chosen (which depends on the order of the cases), the centers are still binary data. For the first iteration, as the cases are compared to cluster means, they will always be at some integer distance from each of the centers. There will often be ties, and the case will be assigned to a cluster in an arbitrary manner. Using Euclidean distance (the only measure available to K-Means), it is impossible to overcome the symmetry and break the ties in any meaningful way.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means No Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/TO_TEST/HF/HF_S_FAnova_extra_outlierTrim_IQS_B.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "numeric_vars = data.select_dtypes(include='number').columns\n",
    "\n",
    "numeric_data = data\n",
    "binary_data = data\n",
    "\n",
    "for n in range(len(numeric_vars)):\n",
    "    num_unique = len(list(set(data[numeric_vars[n]].values)))\n",
    "    if num_unique == 2 or num_unique == 1:\n",
    "        numeric_data = numeric_data.drop(columns=[data.columns[n]], axis=1) #Remove binary columns\n",
    "    else:\n",
    "        binary_data = binary_data.drop(columns=[data.columns[n]], axis=1) #Remove non-binary columns\n",
    "\n",
    "print(numeric_data.head())\n",
    "data = numeric_data\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (6,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Failure Prediction Dataset - Standardized + Importance + FG + Outlier + Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/TO_TEST/HF/HR_S_FImp_extra_outlierTrim_IQS_B.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Yes, it is unlikely that binary data can be clustered satisfactorily. To see why, consider what happens as the K-Means algorithm processes cases.\n",
    "\n",
    "For binary data, the Euclidean distance measure used by K-Means reduces to counting the number of variables on which two cases disagree. After the initial centers are chosen (which depends on the order of the cases), the centers are still binary data. For the first iteration, as the cases are compared to cluster means, they will always be at some integer distance from each of the centers. There will often be ties, and the case will be assigned to a cluster in an arbitrary manner. Using Euclidean distance (the only measure available to K-Means), it is impossible to overcome the symmetry and break the ties in any meaningful way.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means No Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/TO_TEST/HF/HR_S_FImp_extra_outlierTrim_IQS_B.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "numeric_vars = data.select_dtypes(include='number').columns\n",
    "\n",
    "numeric_data = data\n",
    "binary_data = data\n",
    "\n",
    "for n in range(len(numeric_vars)):\n",
    "    num_unique = len(list(set(data[numeric_vars[n]].values)))\n",
    "    if num_unique == 2 or num_unique == 1:\n",
    "        numeric_data = numeric_data.drop(columns=[data.columns[n]], axis=1) #Remove binary columns\n",
    "    else:\n",
    "        binary_data = binary_data.drop(columns=[data.columns[n]], axis=1) #Remove non-binary columns\n",
    "\n",
    "print(numeric_data.head())\n",
    "data = numeric_data\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (6,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Failure Prediction Dataset - Standardized + Mixed + Outlier + Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/TO_TEST/HF/HR_S_FMixed_outlierTrim_IQS_B.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Yes, it is unlikely that binary data can be clustered satisfactorily. To see why, consider what happens as the K-Means algorithm processes cases.\n",
    "\n",
    "For binary data, the Euclidean distance measure used by K-Means reduces to counting the number of variables on which two cases disagree. After the initial centers are chosen (which depends on the order of the cases), the centers are still binary data. For the first iteration, as the cases are compared to cluster means, they will always be at some integer distance from each of the centers. There will often be ties, and the case will be assigned to a cluster in an arbitrary manner. Using Euclidean distance (the only measure available to K-Means), it is impossible to overcome the symmetry and break the ties in any meaningful way.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means No Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/TO_TEST/HF/HR_S_FMixed_outlierTrim_IQS_B.csv')\n",
    "data.pop('DEATH_EVENT') #Remove target variable\n",
    "numeric_vars = data.select_dtypes(include='number').columns\n",
    "\n",
    "numeric_data = data\n",
    "binary_data = data\n",
    "\n",
    "for n in range(len(numeric_vars)):\n",
    "    num_unique = len(list(set(data[numeric_vars[n]].values)))\n",
    "    if num_unique == 2 or num_unique == 1:\n",
    "        numeric_data = numeric_data.drop(columns=[data.columns[n]], axis=1) #Remove binary columns\n",
    "    else:\n",
    "        binary_data = binary_data.drop(columns=[data.columns[n]], axis=1) #Remove non-binary columns\n",
    "\n",
    "print(numeric_data.head())\n",
    "data = numeric_data\n",
    "\n",
    "N_CLUSTERS = [2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30]\n",
    "rows, cols = (3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse: list = []\n",
    "sc: list = []\n",
    "db: list = []\n",
    "    \n",
    "for n in range(len(N_CLUSTERS)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5), squeeze=False)\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    k = N_CLUSTERS[n]\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    estimator.fit(data)\n",
    "    mse.append(estimator.inertia_)\n",
    "    sc.append(silhouette_score(data, estimator.labels_))\n",
    "    db.append(davies_bouldin_score(data, estimator.labels_))\n",
    "    \n",
    "    print(\"K - \" + str(n))\n",
    "    \n",
    "    for f1 in range (len(data.columns)):\n",
    "        for f2 in range(f1+1, len(data.columns)): \n",
    "            ds.plot_clusters(data, f2, f1, estimator.labels_.astype(float), estimator.cluster_centers_, k,\n",
    "                             f'KMeans k={k}', ax=axs[i,j])\n",
    "            \n",
    "            i, j = (i + 1, 0) if (j+1) % cols == 0 else (i, j + 1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "ds.plot_line(N_CLUSTERS, mse, title='KMeans MSE', xlabel='k', ylabel='MSE')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3), squeeze=False)\n",
    "ds.plot_line(N_CLUSTERS, sc, title='KMeans SC', xlabel='k', ylabel='SC', ax=ax[0, 0], percentage=True)\n",
    "ds.plot_line(N_CLUSTERS, db, title='KMeans DB', xlabel='k', ylabel='DB', ax=ax[0, 1], percentage=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
