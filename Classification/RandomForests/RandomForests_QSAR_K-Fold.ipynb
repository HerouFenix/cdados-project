{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests - Oral Toxicity Dataset\n",
    "## K-Fold Cross Validation\n",
    "_The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import ds_functions as ds\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/qsar_oral_toxicity.csv', sep=';', header=None)    \n",
    "y: np.ndarray = data.pop(1024).values # Target Variable\n",
    "X: np.ndarray = data.values # Values of each feature on each record\n",
    "labels = pd.unique(y)\n",
    "\n",
    "N_SPLITS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = N_SPLITS, shuffle=True)\n",
    "\n",
    "max_depths = [5, 10, 25]\n",
    "n_estimators = [5, 10, 25, 50, 75, 100, 150, 200, 250, 300]\n",
    "max_features = [.1, .3, .5, .7, .9, 1]\n",
    "best_rand_forest = ('', 0, 0)\n",
    "last_best_rf = 0\n",
    "best_rf = None\n",
    "\n",
    "cols = len(max_depths)\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, cols, figsize=(cols*ds.HEIGHT, ds.HEIGHT), squeeze=False)\n",
    "for k in range(len(max_depths)):\n",
    "    d = max_depths[k]\n",
    "    values = {}\n",
    "    for f in max_features:\n",
    "        yvalues = []\n",
    "        for n in n_estimators:\n",
    "            rf = RandomForestClassifier(n_estimators=n, max_depth=d, max_features=f)\n",
    "          \n",
    "            yvalues.append(cross_val_score(rf, X, y, cv=kf).mean())\n",
    "            \n",
    "            if yvalues[-1] > last_best_rf:\n",
    "                best_rand_forest = (d, f, n)\n",
    "                last_best_rf = yvalues[-1]\n",
    "                best_rf = rf\n",
    "\n",
    "        values[f] = yvalues\n",
    "    ds.multiple_line_chart(n_estimators, values, ax=axs[0, k], title='Random Forests with max_depth=%d'%d,\n",
    "                           xlabel='nr estimators', ylabel='accuracy', percentage=True)\n",
    "\n",
    "plt.show()\n",
    "print('Best results with depth=%d, %1.2f features and %d estimators, with accuracy=%1.2f'%(best_rand_forest[0], best_rand_forest[1], best_rand_forest[2], last_best_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees\n",
    "\n",
    "In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_extr_tree = ('', 0, 0)\n",
    "last_best_xt = 0\n",
    "best_xt = None\n",
    "\n",
    "cols = len(max_depths)\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, cols, figsize=(cols*ds.HEIGHT, ds.HEIGHT), squeeze=False)\n",
    "for k in range(len(max_depths)):\n",
    "    d = max_depths[k]\n",
    "    values = {}\n",
    "    for f in max_features:\n",
    "        yvalues = []\n",
    "        for n in n_estimators:\n",
    "            rf = ExtraTreesClassifier(n_estimators=n, max_depth=d, max_features=f)\n",
    "          \n",
    "            yvalues.append(cross_val_score(rf, X, y, cv=kf).mean())\n",
    "            \n",
    "            if yvalues[-1] > last_best_xt:\n",
    "                best_extr_tree = (d, f, n)\n",
    "                last_best_xt = yvalues[-1]\n",
    "                best_xt = rf\n",
    "\n",
    "        values[f] = yvalues\n",
    "    ds.multiple_line_chart(n_estimators, values, ax=axs[0, k], title='Extremely Randomized Trees with max_depth=%d'%d,\n",
    "                           xlabel='nr estimators', ylabel='accuracy', percentage=True)\n",
    "\n",
    "plt.show()\n",
    "print('Best results with depth=%d, %1.2f features and %d estimators, with accuracy=%1.2f'%(best_extr_tree[0], best_extr_tree[1], best_extr_tree[2], last_best_xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0, 0]\n",
    "recall = [0, 0]\n",
    "specificity = [0, 0]\n",
    "precision = [0, 0]\n",
    "matrices = np.zeros((2, 2, N_SPLITS))\n",
    "h = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    trnX, tstX = X[train_index], X[test_index]\n",
    "    trnY, tstY = y[train_index], y[test_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=best_rand_forest[2], max_depth=best_rand_forest[0], max_features=best_rand_forest[1])\n",
    "    rf.fit(trnX, trnY)\n",
    "    \n",
    "    prd_trn = rf.predict(trnX)\n",
    "    prd_tst = rf.predict(tstX)\n",
    "    \n",
    "    cnf_mtx_trn = metrics.confusion_matrix(trnY, prd_trn, labels)\n",
    "    tn_trn, fp_trn, fn_trn, tp_trn = cnf_mtx_trn.ravel()\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    matrices[:,:,h] = cnf_mtx_tst\n",
    "    tn_tst, fp_tst, fn_tst, tp_tst = cnf_mtx_tst.ravel()\n",
    "    \n",
    "    acc[0] += (tn_trn + tp_trn) / (tn_trn + tp_trn + fp_trn + fn_trn)\n",
    "    acc[1] += (tn_tst + tp_tst) / (tn_tst + tp_tst + fp_tst + fn_tst)\n",
    "    \n",
    "    recall[0] += tp_trn / (tp_trn + fn_trn)\n",
    "    recall[1] += tp_tst / (tp_tst + fn_tst)\n",
    "    \n",
    "    specificity[0] += tn_trn / (tn_trn + fp_trn)\n",
    "    specificity[1] += tn_tst / (tn_tst + fp_tst)\n",
    "    \n",
    "    precision[0] += tp_trn / (tp_trn + fp_trn)\n",
    "    precision[1] += tp_tst / (tp_tst + fp_tst)\n",
    "    \n",
    "    h += 1\n",
    "    \n",
    "acc = np.divide(acc, N_SPLITS)\n",
    "recall = np.divide(recall, N_SPLITS)\n",
    "specificity = np.divide(specificity, N_SPLITS)\n",
    "precision = np.divide(precision, N_SPLITS)\n",
    "cnf_mtx_rt = np.mean(matrices, axis=2).astype('int64')\n",
    "\n",
    "evaluation_rt = {'Accuracy': acc,\n",
    "              'Recall': recall,\n",
    "              'Specificity': specificity,\n",
    "              'Precision': precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0, 0]\n",
    "recall = [0, 0]\n",
    "specificity = [0, 0]\n",
    "precision = [0, 0]\n",
    "matrices = np.zeros((2, 2, N_SPLITS))\n",
    "h = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    trnX, tstX = X[train_index], X[test_index]\n",
    "    trnY, tstY = y[train_index], y[test_index]\n",
    "    \n",
    "    et = ExtraTreesClassifier(n_estimators=best_extr_tree[2], max_depth=best_extr_tree[0], max_features=best_extr_tree[1])\n",
    "    et.fit(trnX, trnY)\n",
    "    \n",
    "    prd_trn = et.predict(trnX)\n",
    "    prd_tst = et.predict(tstX)\n",
    "    \n",
    "    cnf_mtx_trn = metrics.confusion_matrix(trnY, prd_trn, labels)\n",
    "    tn_trn, fp_trn, fn_trn, tp_trn = cnf_mtx_trn.ravel()\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    matrices[:,:,h] = cnf_mtx_tst\n",
    "    tn_tst, fp_tst, fn_tst, tp_tst = cnf_mtx_tst.ravel()\n",
    "    \n",
    "    acc[0] += (tn_trn + tp_trn) / (tn_trn + tp_trn + fp_trn + fn_trn)\n",
    "    acc[1] += (tn_tst + tp_tst) / (tn_tst + tp_tst + fp_tst + fn_tst)\n",
    "    \n",
    "    recall[0] += tp_trn / (tp_trn + fn_trn)\n",
    "    recall[1] += tp_tst / (tp_tst + fn_tst)\n",
    "    \n",
    "    specificity[0] += tn_trn / (tn_trn + fp_trn)\n",
    "    specificity[1] += tn_tst / (tn_tst + fp_tst)\n",
    "    \n",
    "    precision[0] += tp_trn / (tp_trn + fp_trn)\n",
    "    precision[1] += tp_tst / (tp_tst + fp_tst)\n",
    "    \n",
    "    h += 1\n",
    "    \n",
    "acc = np.divide(acc, N_SPLITS)\n",
    "recall = np.divide(recall, N_SPLITS)\n",
    "specificity = np.divide(specificity, N_SPLITS)\n",
    "precision = np.divide(precision, N_SPLITS)\n",
    "cnf_mtx = np.mean(matrices, axis=2).astype('int64')\n",
    "\n",
    "evaluation = {'Accuracy': acc,\n",
    "              'Recall': recall,\n",
    "              'Specificity': specificity,\n",
    "              'Precision': precision}\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(2 * ds.HEIGHT, ds.HEIGHT))\n",
    "ds.multiple_bar_chart(['Train', 'Test'], evaluation_rt, ax=axs[0], title=\"Model's performance over Train and Test sets (Random Forests)\")\n",
    "ds.plot_confusion_matrix(cnf_mtx_rt, labels, ax=axs[1])\n",
    "\n",
    "print(\"Random Forests:\\n\", evaluation_rt)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(2 * ds.HEIGHT, ds.HEIGHT))\n",
    "ds.multiple_bar_chart(['Train', 'Test'], evaluation, ax=axs[0], title=\"Model's performance over Train and Test sets (Extremely Random Trees)\")\n",
    "ds.plot_confusion_matrix(cnf_mtx, labels, ax=axs[1])\n",
    "\n",
    "print(\"\\nExtremely Random Trees:\\n\", evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import ds_functions as ds\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('../../datasets/qsar_oral_toxicity.csv', sep=';', header=None)    \n",
    "y: np.ndarray = data.pop(1024).values # Target Variable\n",
    "X: np.ndarray = data.values # Values of each feature on each record\n",
    "labels = pd.unique(y)\n",
    "\n",
    "N_SPLITS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = N_SPLITS, shuffle=True)\n",
    "\n",
    "max_depths = [5, 10, 25]\n",
    "n_estimators = [5, 10, 25, 50, 75, 100, 150, 200, 250, 300]\n",
    "max_features = [.1, .3, .5, .7, .9, 1]\n",
    "best_rand_forest = ('', 0, 0)\n",
    "last_best_rf = 0\n",
    "best_rf = None\n",
    "\n",
    "cols = len(max_depths)\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, cols, figsize=(cols*ds.HEIGHT, ds.HEIGHT), squeeze=False)\n",
    "for k in range(len(max_depths)):\n",
    "    d = max_depths[k]\n",
    "    values = {}\n",
    "    for f in max_features:\n",
    "        yvalues = []\n",
    "        for n in n_estimators:\n",
    "            rf = RandomForestClassifier(n_estimators=n, max_depth=d, max_features=f)\n",
    "          \n",
    "            yvalues.append(cross_val_score(rf, X, y, cv=kf).mean())\n",
    "            \n",
    "            if yvalues[-1] > last_best_rf:\n",
    "                best_rand_forest = (d, f, n)\n",
    "                last_best_rf = yvalues[-1]\n",
    "                best_rf = rf\n",
    "\n",
    "        values[f] = yvalues\n",
    "    ds.multiple_line_chart(n_estimators, values, ax=axs[0, k], title='Random Forests with max_depth=%d'%d,\n",
    "                           xlabel='nr estimators', ylabel='accuracy', percentage=True)\n",
    "\n",
    "plt.show()\n",
    "print('Best results with depth=%d, %1.2f features and %d estimators, with accuracy=%1.2f'%(best_rand_forest[0], best_rand_forest[1], best_rand_forest[2], last_best_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees\n",
    "\n",
    "In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_extr_tree = ('', 0, 0)\n",
    "last_best_xt = 0\n",
    "best_xt = None\n",
    "\n",
    "cols = len(max_depths)\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, cols, figsize=(cols*ds.HEIGHT, ds.HEIGHT), squeeze=False)\n",
    "for k in range(len(max_depths)):\n",
    "    d = max_depths[k]\n",
    "    values = {}\n",
    "    for f in max_features:\n",
    "        yvalues = []\n",
    "        for n in n_estimators:\n",
    "            rf = ExtraTreesClassifier(n_estimators=n, max_depth=d, max_features=f)\n",
    "          \n",
    "            yvalues.append(cross_val_score(rf, X, y, cv=kf).mean())\n",
    "            \n",
    "            if yvalues[-1] > last_best_xt:\n",
    "                best_extr_tree = (d, f, n)\n",
    "                last_best_xt = yvalues[-1]\n",
    "                best_xt = rf\n",
    "\n",
    "        values[f] = yvalues\n",
    "    ds.multiple_line_chart(n_estimators, values, ax=axs[0, k], title='Extremely Randomized Trees with max_depth=%d'%d,\n",
    "                           xlabel='nr estimators', ylabel='accuracy', percentage=True)\n",
    "\n",
    "plt.show()\n",
    "print('Best results with depth=%d, %1.2f features and %d estimators, with accuracy=%1.2f'%(best_extr_tree[0], best_extr_tree[1], best_extr_tree[2], last_best_xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0, 0]\n",
    "recall = [0, 0]\n",
    "specificity = [0, 0]\n",
    "precision = [0, 0]\n",
    "matrices = np.zeros((2, 2, N_SPLITS))\n",
    "h = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    trnX, tstX = X[train_index], X[test_index]\n",
    "    trnY, tstY = y[train_index], y[test_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=best_rand_forest[2], max_depth=best_rand_forest[0], max_features=best_rand_forest[1])\n",
    "    rf.fit(trnX, trnY)\n",
    "    \n",
    "    prd_trn = rf.predict(trnX)\n",
    "    prd_tst = rf.predict(tstX)\n",
    "    \n",
    "    cnf_mtx_trn = metrics.confusion_matrix(trnY, prd_trn, labels)\n",
    "    tn_trn, fp_trn, fn_trn, tp_trn = cnf_mtx_trn.ravel()\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    matrices[:,:,h] = cnf_mtx_tst\n",
    "    tn_tst, fp_tst, fn_tst, tp_tst = cnf_mtx_tst.ravel()\n",
    "    \n",
    "    acc[0] += (tn_trn + tp_trn) / (tn_trn + tp_trn + fp_trn + fn_trn)\n",
    "    acc[1] += (tn_tst + tp_tst) / (tn_tst + tp_tst + fp_tst + fn_tst)\n",
    "    \n",
    "    recall[0] += tp_trn / (tp_trn + fn_trn)\n",
    "    recall[1] += tp_tst / (tp_tst + fn_tst)\n",
    "    \n",
    "    specificity[0] += tn_trn / (tn_trn + fp_trn)\n",
    "    specificity[1] += tn_tst / (tn_tst + fp_tst)\n",
    "    \n",
    "    precision[0] += tp_trn / (tp_trn + fp_trn)\n",
    "    precision[1] += tp_tst / (tp_tst + fp_tst)\n",
    "    \n",
    "    h += 1\n",
    "    \n",
    "acc = np.divide(acc, N_SPLITS)\n",
    "recall = np.divide(recall, N_SPLITS)\n",
    "specificity = np.divide(specificity, N_SPLITS)\n",
    "precision = np.divide(precision, N_SPLITS)\n",
    "cnf_mtx_rt = np.mean(matrices, axis=2).astype('int64')\n",
    "\n",
    "evaluation_rt = {'Accuracy': acc,\n",
    "              'Recall': recall,\n",
    "              'Specificity': specificity,\n",
    "              'Precision': precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0, 0]\n",
    "recall = [0, 0]\n",
    "specificity = [0, 0]\n",
    "precision = [0, 0]\n",
    "matrices = np.zeros((2, 2, N_SPLITS))\n",
    "h = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    trnX, tstX = X[train_index], X[test_index]\n",
    "    trnY, tstY = y[train_index], y[test_index]\n",
    "    \n",
    "    et = ExtraTreesClassifier(n_estimators=best_extr_tree[2], max_depth=best_extr_tree[0], max_features=best_extr_tree[1])\n",
    "    et.fit(trnX, trnY)\n",
    "    \n",
    "    prd_trn = et.predict(trnX)\n",
    "    prd_tst = et.predict(tstX)\n",
    "    \n",
    "    cnf_mtx_trn = metrics.confusion_matrix(trnY, prd_trn, labels)\n",
    "    tn_trn, fp_trn, fn_trn, tp_trn = cnf_mtx_trn.ravel()\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    matrices[:,:,h] = cnf_mtx_tst\n",
    "    tn_tst, fp_tst, fn_tst, tp_tst = cnf_mtx_tst.ravel()\n",
    "    \n",
    "    acc[0] += (tn_trn + tp_trn) / (tn_trn + tp_trn + fp_trn + fn_trn)\n",
    "    acc[1] += (tn_tst + tp_tst) / (tn_tst + tp_tst + fp_tst + fn_tst)\n",
    "    \n",
    "    recall[0] += tp_trn / (tp_trn + fn_trn)\n",
    "    recall[1] += tp_tst / (tp_tst + fn_tst)\n",
    "    \n",
    "    specificity[0] += tn_trn / (tn_trn + fp_trn)\n",
    "    specificity[1] += tn_tst / (tn_tst + fp_tst)\n",
    "    \n",
    "    precision[0] += tp_trn / (tp_trn + fp_trn)\n",
    "    precision[1] += tp_tst / (tp_tst + fp_tst)\n",
    "    \n",
    "    h += 1\n",
    "    \n",
    "acc = np.divide(acc, N_SPLITS)\n",
    "recall = np.divide(recall, N_SPLITS)\n",
    "specificity = np.divide(specificity, N_SPLITS)\n",
    "precision = np.divide(precision, N_SPLITS)\n",
    "cnf_mtx = np.mean(matrices, axis=2).astype('int64')\n",
    "\n",
    "evaluation = {'Accuracy': acc,\n",
    "              'Recall': recall,\n",
    "              'Specificity': specificity,\n",
    "              'Precision': precision}\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(2 * ds.HEIGHT, ds.HEIGHT))\n",
    "ds.multiple_bar_chart(['Train', 'Test'], evaluation_rt, ax=axs[0], title=\"Model's performance over Train and Test sets (Random Forests)\")\n",
    "ds.plot_confusion_matrix(cnf_mtx_rt, labels, ax=axs[1])\n",
    "\n",
    "print(\"Random Forests:\\n\", evaluation_rt)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(2 * ds.HEIGHT, ds.HEIGHT))\n",
    "ds.multiple_bar_chart(['Train', 'Test'], evaluation, ax=axs[0], title=\"Model's performance over Train and Test sets (Extremely Random Trees)\")\n",
    "ds.plot_confusion_matrix(cnf_mtx, labels, ax=axs[1])\n",
    "\n",
    "print(\"\\nExtremely Random Trees:\\n\", evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
